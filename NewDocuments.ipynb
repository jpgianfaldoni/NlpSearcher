{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('universal_tagset')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.colors as mcolors\n",
    "plt.style.use('default')\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import NMF, PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import Isomap, TSNE\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, TimeDistributed, Embedding, GlobalAveragePooling1D, Flatten, SimpleRNN, GRU, Dropout, LSTM\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report\n",
    "import requests\n",
    "stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "stopwords = set(stopwords_list.decode().splitlines()) \n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DISCLAIMER**\n",
    "#### A ideia para essa APS era utilizar a série de livros Harry Potter como dataset, sendo que cada página representa um documento, e os documentos são classificados como positivos ou negativos, dependendo da presença ou não de personagens antagonistas.\n",
    "#### A partir desses dados seriam gerados entidades nomeadas, e 2 redes neurais seriam treinadas para gerar novos documentos, uma para os de classificação positiva, e uma para os de negativa.\n",
    "#### Foi possível realizar a parte teórica, mas por diversas limitações de processamento e dados as redes não produziram resultados muito satisfatórios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING DATA\n",
    "\n",
    "with open(\"Book1.txt\", encoding=\"utf8\") as f:\n",
    "    Book1 = f.readlines()\n",
    "with open(\"Book2.txt\", encoding=\"utf8\") as f:\n",
    "    Book2 = f.readlines()\n",
    "# with open(\"Book3.txt\", encoding=\"utf8\") as f:\n",
    "#     Book3 = f.readlines()\n",
    "# with open(\"Book4.txt\", encoding=\"utf8\") as f:\n",
    "#     Book4 = f.readlines()\n",
    "# with open(\"Book5.txt\", encoding=\"utf8\") as f:\n",
    "#     Book5 = f.readlines()\n",
    "# with open(\"Book6.txt\", encoding=\"utf8\") as f:\n",
    "#     Book6 = f.readlines()\n",
    "# with open(\"Book7.txt\", encoding=\"utf8\") as f:\n",
    "#     Book7 = f.readlines()\n",
    "# Books = [Book1, Book2, Book3, Book4, Book5, Book6, Book7]\n",
    "Books = [Book1, Book2]\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA CLEANING\n",
    "AllBooksText = []\n",
    "BookTitles = ['Harry Potter and the Philosophers Stone - J.K. Rowling', 'Harry Potter and the Order of the Phoenix - J.K. Rowling', 'Harry Potter and the Half Blood Prince - J.K. Rowling', 'Harry Potter and the Deathly Hallows - J.K. Rowling']\n",
    "for b in Books:\n",
    "    b  = ''.join(map(str, b))\n",
    "    b =  b.replace('\\n', '')\n",
    "    for bt in BookTitles:\n",
    "        b = b.replace(bt, '')\n",
    "    bPages = b.split('Page |')\n",
    "    AllBooksText.append(bPages)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllBooksText = [item for sublist in AllBooksText for item in sublist]\n",
    "AllBooksTextClean = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in AllBooksText:\n",
    "    for bt in BookTitles:\n",
    "        pclean = p.replace(bt, '')\n",
    "    AllBooksTextClean.append(pclean)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS TAGGING\n",
    "pos_tagged = []\n",
    "\n",
    "\n",
    "for doc in AllBooksTextClean:\n",
    "    try:\n",
    "        doc = re.sub(r'[^A-Za-z0-9 ]+', '', doc)\n",
    "        wt = nltk.word_tokenize(doc)\n",
    "        pos_tagged.append(nltk.pos_tag(wt, tagset = \"universal\"))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SENTIMENT CLASSIFICATION AND CREATING NAMED ENTITIES\n",
    "evilCharacters = ['Voldemort', 'Snape', 'Dursley', 'Malfoy', 'Dudley', 'Tom']\n",
    "pattern = 'NamedEntity: {<NOUN><NOUN>}'\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "NamedEntities = []\n",
    "testList = []\n",
    "cbad = 0\n",
    "cgood = 0\n",
    "bad = False\n",
    "i = 0\n",
    "finalList = []\n",
    "for p in pos_tagged:\n",
    "    testList = []\n",
    "    for x in p:\n",
    "        for eC in evilCharacters:\n",
    "            if eC in x:\n",
    "                bad = True\n",
    "    cs = cp.parse(p)\n",
    "    for p in cs:\n",
    "        xlist = []\n",
    "        try:\n",
    "            if p.label() == \"NamedEntity\":\n",
    "                xlist.append(' '.join([i[0] for i in p.leaves()]))\n",
    "                xlist.append(AllBooksTextClean[i])\n",
    "                if bad:\n",
    "                    xlist.append(\"EVIL\")\n",
    "                else:\n",
    "                    xlist.append(\"GOOD\")\n",
    "        except AttributeError:\n",
    "            continue\n",
    "        testList.append(xlist)\n",
    "    bad = False\n",
    "    finalList.append(testList)\n",
    "    i += 1\n",
    "            \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = []\n",
    "texts = []\n",
    "sentiments = []\n",
    "for i in finalList:\n",
    "    for j in i:\n",
    "        entities.append(j[0])\n",
    "        texts.append(j[1])\n",
    "        sentiments.append(j[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\"entities\":entities, \"texts\": texts, \"sentiments\": sentiments}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dictionary)\n",
    "positives = df[df[\"sentiments\"] == \"GOOD\"]\n",
    "negatives = df[df[\"sentiments\"] == \"EVIL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positives.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_entities = positives[\"entities\"]\n",
    "p_labels = positives[\"texts\"]\n",
    "n_entities = negatives[\"entities\"]\n",
    "n_labels = negatives[\"texts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOKENIZING, PADDING AND ENCODING\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokens_entities = Tokenizer(num_words=128)\n",
    "tokenizer = Tokenizer(num_words=128)\n",
    "tokenizer.fit_on_texts(positives[\"texts\"])\n",
    "padded_seq = pad_sequences(tokenizer.texts_to_sequences(positives[\"texts\"]), maxlen=128)\n",
    "tokens_entities.fit_on_texts(positives[\"entities\"])\n",
    "X = pad_sequences(tokens_entities.texts_to_sequences(positives[\"entities\"]), maxlen=128)\n",
    "y_ = np.array(padded_seq)\n",
    "y_ = y_.reshape(y_.shape[0]*y_.shape[1],1)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y = onehot_encoder.fit_transform(y_)\n",
    "y = y.reshape(padded_seq.shape[0], 128, X.shape[1])\n",
    "print(f'{X.shape=} {y.shape=}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = Tokenizer(num_words=128)\n",
    "tokenizer2.fit_on_texts(negatives[\"texts\"])\n",
    "padded_seq = pad_sequences(tokenizer2.texts_to_sequences(negatives[\"texts\"]), maxlen=128)\n",
    "tokens_entities.fit_on_texts(negatives[\"entities\"])\n",
    "X2 = pad_sequences(tokens_entities.texts_to_sequences(negatives[\"entities\"]), maxlen=128)\n",
    "y_2 = np.array(padded_seq)\n",
    "y_2 = y_2.reshape(y_2.shape[0]*y_2.shape[1],1)\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y2 = onehot_encoder.fit_transform(y_2)\n",
    "y2 = y2.reshape(padded_seq.shape[0], 128, X.shape[1])\n",
    "print(f'{X2.shape=} {y2.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN/TEST SPLIT\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(int)\n",
    "Y = y.astype(int)\n",
    "print((X.shape[1], Y.shape[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X2.astype(int)\n",
    "Y2 = y2.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Model, load_model\n",
    "# rede_neural_positiva = load_model('redePositiva')\n",
    "# rede_neural_negativa = load_model('redeNegativa')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEURAL NETWORK STRUCTURE\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "def rede_neural_geracao(input_dims, dim_out):\n",
    "        input_layer = Input(shape=(input_dims,))\n",
    "        x = input_layer\n",
    "        x = Embedding(150, 150, name='projecao')(x)\n",
    "        x, state_h, state_c = LSTM(512, return_sequences=True, return_state=True)(x)\n",
    "        x = LSTM(512, return_sequences=True) (x, initial_state=[state_h, state_c])\n",
    "        x = Dropout(0.1)(x)\n",
    "        y = Dense(dim_out, activation='softmax', name='decisor')(x)\n",
    "        return Model(input_layer, y)\n",
    "rede_neural = rede_neural_geracao(X.shape[1], Y.shape[1])\n",
    "rede_neural.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "plot_model(rede_neural, show_shapes=True, show_layer_activations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positive_prediction(entity: str):\n",
    "    X = pad_sequences(tokens_entities.texts_to_sequences([entity]), maxlen=128)\n",
    "    pred = rede_neural_positiva.predict(X)\n",
    "    return tokenizer.sequences_to_texts(np.argmax(pred, axis=1))\n",
    "\n",
    "def negative_prediction(entity: str):\n",
    "    X = pad_sequences(tokens_entities.texts_to_sequences([entity]), maxlen=128)\n",
    "    pred = rede_neural_negativa.predict(X)\n",
    "    return tokenizer.sequences_to_texts(np.argmax(pred, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_prediction(\"Harry Potter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_prediction(\"Severus Snape Lord Voldemort\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d0a4d82deee81db9a0d09e308519aed460fc6b5372e2dfd7660684eac61088a6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
