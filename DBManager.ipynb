{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "con = sqlite3.connect('SQL.db')\n",
    "cur = con.cursor()\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialmente criamos a tabela words, que irá conter todas as palavras do nosso vocabulário. Em sequência criamos a tabela website_has_word, que é uma tabela que relaciona uma determinada palavra a um site, e seu respectivo Tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cur.execute(\"CREATE TABLE words (word_id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT, word TEXT)\")\n",
    "# cur.execute(\"CREATE TABLE website_has_word (whw_id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT, word_id INT, website_id INT, tfidf FLOAT)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A partir de todos os textos disponíveis, criamos a matriz esparsa de Tfidfs e a transformamos em um dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for r in cur.execute(\"SELECT website_id, website_content FROM websites\"):\n",
    "    website_id = r[0]\n",
    "    website_content = r[1]\n",
    "    all_data.append(r[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(min_df = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vec.fit_transform(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df = pd.DataFrame(data.toarray(), columns=vec.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com esse dataframe criamos um set com todas palavras únicas, removendo símbolos e caracteres indesejados, e as inserimos na tabela word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set()\n",
    "for i in tfidf_df.columns:\n",
    "    clean_word = re.sub(\"[^A-Za-z#(\\w+)]\", \"\", i);\n",
    "    all_words.add(clean_word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in all_words:\n",
    "    cur.execute(f\"INSERT INTO words (word) VALUES ('{word}')\")\n",
    "    con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalmente, utilizamos o dataframe de Tfidfs e a tabela words para preencher a tabela website_has_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = {}\n",
    "for word in tfidf_df.columns:\n",
    "    r =cur.execute(f\"SELECT word_id FROM words WHERE word = '{word}'\")\n",
    "    s = cur.fetchall()\n",
    "    if len(s) > 0:\n",
    "        word_ids[word] = s[0][0]\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_var = []\n",
    "i = 1\n",
    "for index, row in tfidf_df.iterrows():\n",
    "    for e in range(len(row)):\n",
    "        if row[e] != 0:\n",
    "            clean_word = re.sub(\"[^A-Za-z#(\\w+)]\", \"\", row.index[e]);\n",
    "            insert_var.append((word_ids[clean_word], i, row[e]))\n",
    "    insert_var_map = ','.join(list(map(str ,insert_var)))\n",
    "    if len(insert_var_map) != 0:\n",
    "        cur.execute(f\"INSERT INTO website_has_word (word_id, website_id, tfidf) VALUES {insert_var_map}\")\n",
    "        con.commit()\n",
    "        insert_var = []\n",
    "        print(f\"Document{i} finished\")\n",
    "    i+=1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
