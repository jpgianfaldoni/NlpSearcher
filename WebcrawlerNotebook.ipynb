{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "from functools import partial\n",
    "import mysql.connector\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para coleta de URLs, títulos, e textos das páginas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPageSoup(url):\n",
    "    user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'\n",
    "    headers={'User-Agent':user_agent,} \n",
    "    request=urllib.request.Request(url,None,headers) \n",
    "    response = urllib.request.urlopen(request)\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def getPageText(soup):\n",
    "    page_text = soup.find_all('p')\n",
    "    page_string = \"\"\n",
    "    for p in page_text:\n",
    "        page_string += p.get_text()\n",
    "    return page_string\n",
    "\n",
    "def getPageTitle(soup):\n",
    "    return soup.title.string\n",
    "\n",
    "def getAllLinks(soup):\n",
    "    links =[]\n",
    "    all_links = soup.find_all('a')\n",
    "    for link in all_links:\n",
    "        links.append(link.get('href'))\n",
    "    return links\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaração da URL e variáveis iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_url = \"https://johnpistelli.com/\"\n",
    "soup = getPageSoup(original_url)\n",
    "page_links = getAllLinks(soup)\n",
    "page_links_set = set()\n",
    "page_links_set.add(original_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop inicial para preencher o Set com todos os links encontrados pelo crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_visited_pages = 50000 # número de links máximo que serão percorridos\n",
    "counter = 0\n",
    "for page in page_links:\n",
    "    if counter > max_visited_pages:\n",
    "        break\n",
    "    try:\n",
    "        soup = getPageSoup(page)\n",
    "        links = getAllLinks(soup)\n",
    "        for link in links:\n",
    "            page_links_set.add(link)\n",
    "            counter += 1\n",
    "    except:\n",
    "        print(\"bad request\")\n",
    "        \n",
    "\n",
    "print(\"Set completed\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conexão com o DataBase (mysql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "con = sqlite3.connect('SQL.db')\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação da tabela (rodar apenas uma vez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"DROP TABLE websites\")\n",
    "cur.execute(\"DROP TABLE words\")\n",
    "cur.execute(\"DROP TABLE website_has_word\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"CREATE TABLE websites(website_id INTEGER NOT NULL PRIMARY KEY AUTOINCREMENT, url VARCHAR(150),website_content TEXT,website_title TEXT)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop para o preenchimento do Database com o título, URL e texto de cada página"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in page_links_set:\n",
    "    try:\n",
    "        soup = getPageSoup(link)\n",
    "        page_text = getPageText(soup)\n",
    "        page_title = getPageTitle(soup)\n",
    "        cur.execute(f\"INSERT INTO websites(url, website_content, website_title) VALUES('{link}', '{page_text}', '{page_title}')\")\n",
    "        con.commit()\n",
    "    except:\n",
    "        print(\"An exception occurred\")\n",
    "connection.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
